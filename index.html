<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Transformers in Reinforcement Learning: A Survey">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Transformers in Reinforcement Learning: A Survey</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Transformers in Reinforcement Learning        :  &nbsp;&nbsp; A Survey</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://pranaval.github.io/">Pranav Agarwal</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://aamer98.github.io/"> Aamer Abdul Rahman</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://mila.quebec/en/person/plstcharles/">Pierre-Luc St-Charles</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/simon-prince-615bb9165/?originalSubdomain=ca">Simon J.D. Prince</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://saebrahimi.github.io/">Samira Ebrahimi Kahou</a><sup>1,2,4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>École de Technologie Supérieure,</span>
            <span class="author-block"><sup>2</sup>Mila,</span>
            <span class="author-block"><sup>3</sup>University of Bath,</span>
            <span class="author-block"><sup>4</sup>CIFAR</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.05979.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2307.059798"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/pranavAL/Transformers-in-Reinforcement-Learning"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Full Paper List</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="./static/images/survey-full.gif" alt="elign" style="border-style: none" width="850">
            </td>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transformers have significantly impacted domains like natural language processing, computer vision, and
            robotics, where they improve performance compared to other neural networks. This survey explores how
            transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability.
            We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical
            RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application
            of transformers to various aspects of RL, including representation learning, transition and reward function
            modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability
            and efficiency of transformers in RL, using visualization techniques and efficient training strategies. Often,
            the transformer architecture must be tailored to the specific needs of a given application. We present a broad
            overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization. We conclude by discussing the limitations
            of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{agarwal2023transformers,
      title={Transformers in reinforcement learning: A survey},
      author={Agarwal, Pranav and Rahman, Aamer Abdul and St-Charles, Pierre-Luc and Prince, Simon JD and Kahou, Samira Ebrahimi},
      journal={arXiv preprint arXiv:2307.05979},
      year={2023}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">

    <div class="content has-text-centered">
      <div class="column is-8">
        <div class="content">
          <center>
          <p>
            &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;Page template from a <a href="https://nerfies.github.io/">Nerfies</a>
          </center>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
